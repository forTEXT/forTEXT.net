{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "633cf476",
   "metadata": {},
   "source": [
    "# Lerneinheit Preprocessing mit NLTK\n",
    "\n",
    "Mit dieser Lerneinheit können Sie Texte für die digitale Analyse mit anderen Tools vorbereiten.\n",
    "Wir haben das Preprocessing in zwei Komplexe unterteilt.\n",
    "\n",
    "Im ersten Komplex finden Sie Funktionen, mit denen Sie die Form eines Textes verändern können, ohne dabei inhaltlich einzugreifen.\n",
    "Mit diesen Funktionen können Sie einen Text in einzelne Wörter oder Sätze unterteilen (Tokenisierung auf Wort- oder Satzebene), den gesamten Text in Kleinschreibung umwandeln, und doppelte Leerzeichen und Leerzeilen entfernen.\n",
    "\n",
    "Im zweiten Komplex finden Sie Funktionen, mit denen Sie einzelne Elemente aus einem Text herauslöschen können.\n",
    "Wenn Sie alle Wörter entfernen, die weniger als drei Zeichen haben, so können Sie damit meist Fehler eliminieren, die auf ein suboptimales OCR zurückzuführen sind.\n",
    "Sie können Zahlen und Satzzeichen aus einem Text entfernen.\n",
    "Sie können Sätze mit einer bestimmten Wortanzahl aus dem Text entfernen.\n",
    "Schließlich können Sie auch eine ganze Liste von Stopwords (Wörtern, die Sie in Ihrer Analyse nicht mit einbeziehen möchten) aus dem Text entfernen.\n",
    "\n",
    "Die Lerneinheit ist so aufgebaut, dass Sie jede Funktion einzeln ausführen können.\n",
    "Suchen Sie sich die Funktionen heraus, die Sie für Ihre Korpusvorbereitung brauchen.\n",
    "Funktionen, die Sie nicht benötigen, können Sie einfach überspringen.\n",
    "Wenn ein Arbeitsschritt abgeschlossen ist, erscheint in den eckigen Klammern links an der Seite eine kleine Zahl.\n",
    "Wird ein Arbeitsschritt noch ausgeführt, so sehen Sie hier ein Asterisk (*).\n",
    "Daran erkennen Sie, dass Ihr Computer noch arbeitet.\n",
    "\n",
    "## NLTK laden\n",
    "\n",
    "Bevor Sie mit dem Prepsorcessing Ihres Textes beginnen können, müssen Sie das NLTK-Package (Natural Language Processing Toolkit) und ein paar zusätzliche Packages laden.\n",
    "Klicken Sie dazu in die Box unten, sodass Ihr Cursor dort erscheint.\n",
    "Klicken Sie dann oben im Menü auf \"Run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "565d39e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd38e63a",
   "metadata": {},
   "source": [
    "## Funktionen zur Anpassung der Textform\n",
    "\n",
    "Manche digitalen Methoden und Tools zur Textanalyse benötigen vorverarbeitete Dokumente, die eine ganz bestimmte Form aufweisen.\n",
    "Manchmal benötigen Sie z.B. einen Darstellung, in der in jeder Zeile ein Wort steht oder pro Zeile ein Satz.\n",
    "In der digitalen Stilometrie (mehr darüber in Horstmann 2018) kann es manchmal sinnvoll sein, Texte so vorzubereiten, dass sie ausschließlich Kleinschreibung aufweisen, um die Großschreibung häufiger Wörter wie \"der\", \"die\" oder \"das\" am Satzanfang herauszunehmen.\n",
    "Im Folgenden Abschnitt finden Sie Funktionen des Python-Packages NLTK, mit denen sie solche Operationen durchführen können, die nicht in den eigentlichen Text, sondern nur in dessen Form eingreifen.\n",
    "\n",
    "### Tokenisierung auf Wortebene\n",
    "\n",
    "Mit der Funktion zur Tokenisierung können Sie Elemente in einem Text vereinzeln bzw. einzeln markieren.\n",
    "Mit NLTK können Sie Tokenisierung sowohl auf Wort- als auch auf Satzebene durchführen.\n",
    "Um die Tokenisierung auf Wortebene durchzuführen und den tokenisierten Text in einer neuen Datei zu speichern, klicken Sie nun in die unten stehende Box.\n",
    "Klicken Sie dann oben im Menü auf \"Run\", um den Vorgang zu starten.\n",
    "Sobald links in den eckigen Klammern eine kleine Zahl erscheint, müsste das Programm eine neue Datei mit Ihren Daten in Ihrem Ordnersystem abgelegt haben.\n",
    "\n",
    "Dabei wird jedes Token in einer neuen Textzeile ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b1987e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Downloads/1774-Werther.txt', 'r', encoding='utf-8') as input:\n",
    "    text = input.read()\n",
    "\n",
    "# Tokenisierung\n",
    "tokens = nltk.word_tokenize(text, 'german')\n",
    "\n",
    "# Der join() Befehl verbindet Elemente einer Liste mit einem String.\n",
    "# Wir verwenden hier den Zeilenumbruch '\\n' als Verbindungselement, um eine vertikale Wortliste zu erstellen.\n",
    "output_str = '\\n'.join(tokens)\n",
    "\n",
    "with open('Downloads/Werther_Tokens.txt', 'w', encoding='utf-8') as output:\n",
    "    output.write(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4261b2",
   "metadata": {},
   "source": [
    "### Tokenisierung auf Satzebene\n",
    "\n",
    "Um die Tokenisierung auf Satzebene durchzuführen und den tokenisierten Text in einer neuen Datei zu speichern, klicken Sie nun in die unten stehende Box. Klicken Sie dann oben im Menü auf \"Run\", um den Vorgang zu starten. Sobald links in den eckigen Klammern eine kleine Zahl erscheint, müsste das Programm eine neue Datei mit Ihren Daten in Ihrem Ordnersystem abgelegt haben.\n",
    "\n",
    "Dabei wird jeder Satz in einer neuen Textzeile ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "35d2df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Downloads/1774-Werther.txt', 'r', encoding='utf-8') as input:\n",
    "    text = input.read()\n",
    "    \n",
    "sentences = nltk.sent_tokenize(text,language='german')\n",
    "output_str = '\\n'.join(sentences)\n",
    "    \n",
    "with open('Downloads/Werther_Saetze_neu1.txt', 'w', encoding='utf-8') as output:\n",
    "    output.write(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f541ee93",
   "metadata": {},
   "source": [
    "### Umwandlung in Kleinschreibung\n",
    "\n",
    "Für manche Methoden und / oder manche Sprachen kann es sein, dass es sinnvoll ist, den gesamten Text in Kleinschreibung zu transformieren.\n",
    "Wenn Sie Ihren Text auf diese Weise umwandeln und das Ergebnis in einer neuen Datei speichern möchten, klicken Sie unten in die Box und dann oben im Menü auf \"Run\".\n",
    "Das Programm legt dann eine neue Datei mit dem unten aufgeführten Namen in Ihrem Ordnersystem ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "289164ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Downloads/1774-Werther.txt', 'r', encoding='utf-8') as input:\n",
    "    text = input.read()\n",
    "\n",
    "lower_text = text.lower()\n",
    "\n",
    "with open('Downloads/Werther_Saetze_lowercase1.txt', 'w', encoding='utf-8') as output:\n",
    "    output.write(lower_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1ce4e3",
   "metadata": {},
   "source": [
    "### Doppelte Leerzeichen und Leerzeilen entfernen\n",
    "\n",
    "Manchmal kommen durch die Vorverabeitung oder auch durch die Digitalisierung Leerzeichen und -zeilen in Texte, die Sie vielleicht bei Ihren weiteren Verarbeitungsschritten stören können. Die folgenden Funktionen helfen Ihnen dabei, doppelte Leerzeichen und Leerzeilen zu entfernen.\n",
    "\n",
    "#### Doppelte Leerzeichen entfernen\n",
    "\n",
    "Wenn Sie doppelte Leerzeichen in Ihrem Text entfernen möchten, klicken Sie unten in die Box, sodass Ihr Cursor dort erscheint. Klicken Sie dann oben im Menü auf \"Run\". Sobald links in den eckigen Klammern eine Zahl erscheint, wurde Ihr Ergebnis in einer neuen Datei gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cecd9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Downloads/1774-Werther.txt', 'r', encoding='utf-8') as input:\n",
    "    text = input.read()\n",
    "\n",
    "# Mit der re.sub() Funktion können wir reguläre Ausdrücke verwenden, um bestimmte Textelemente zu entfernen.\n",
    "# Mehrfachvorkommen von Leerzeichen werden mit dem regulärem Ausdruck ' +' gefunden.\n",
    "# Um reguläre Ausdrücke auszuprobieren, kann man zum Beispiel diese Website verwenden: https://regexr.com/\n",
    "clean_text = re.sub(\n",
    "    ' +',   # der reguläre Ausdruck mit dem wir die Zeichen finden, die entfernt werden sollen.\n",
    "    ' ',    # Was eingefügt werden soll. In diesem Fall ein einfaches Leerzeichen\n",
    "    text    # Der Text den wir bereinigen wollen.\n",
    ")\n",
    "\n",
    "with open('Downloads/1774-Werther_ohneLeerzeichen.txt', 'w', encoding='utf-8') as output:\n",
    "    output.write(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f24ae8d",
   "metadata": {},
   "source": [
    "#### Leerzeilen entfernen\n",
    "\n",
    "Wenn Sie Leerzeilen in Ihrem Text entfernen möchten, klicken Sie in die folgende Box und dann oben im Menü auf \"Run\". Sobald links in den eckigen Klammern eine Zahl erscheint, wurde Ihr Ergebnis in einer neuen Datei gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ba3f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Downloads/1774-Werther.txt', 'r', encoding='utf-8') as input:\n",
    "    text = input.read()\n",
    "\n",
    "clean_text = re.sub(\n",
    "    '\\n+',  # Mit diesem regulären Ausdruck suchen wir nach Mehrfachvorkommen von Zeilenumbrüchen.\n",
    "    ' ',\n",
    "    text\n",
    ")\n",
    "\n",
    "with open('Downloads/1774-Werther_ohneLeerzeilen.txt', 'w', encoding='utf-8') as output:\n",
    "    output.write(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d1e6c",
   "metadata": {},
   "source": [
    "## Funktionen zum Herauslöschen einzelner Elemente\n",
    "\n",
    "Sie wissen nun, wie Sie formale Elemente Ihres Textes verändern können. Sie können auf Satz- und Wortebene tokenisieren, durchgängige Kleinschreibung bewirken und doppelte Leerzeichen sowie Leerzeilen entfernen. Für manche Methoden, wie z.B. das Topic Modeling oder auch word2vec kann es sinnvoll sein, bei der Vorverarbeitung auch in den Text selbst einzugreifen. Von großer Bedeutung ist hier die Entfernung von Stopwörtern, d.h. Wörtern, die nicht in die Analyse einbezogen werden sollen (zur Bedeutung von Stopwörtern beim Topic Modeling vgl. Horstmann 2018). Auch das entfernen von Zahlen, Interpunktionszeichen oder kurzen Wörtern oder Sätzen kann für einige Methoden der Digital Humanities sinnvoll sein. Wählen Sie sich die Funktionen, die Sie brauchen einfach aus den folgenden Blöcken aus und überspringen Sie diejenigen, die für Sie nicht sinnvoll sind.\n",
    "\n",
    "Bei manchen Funktionen können Sie wählen, in welchem Format Sie ihr Ergebnis abspeichern wollen. In diesem Fall finden Sie zu einer Funktion mehrere Abschnitte.\n",
    "\n",
    "### Entfernen von Stopwörtern\n",
    "\n",
    "Das NLTK-Package stell auch eine Reihe von Stopwortlisten für verschiedene Sprachen zur Verfügung. Im folgenden Beispiel führen Sie die Entfernung von Stopwörtern mit der Liste deutscher Stopwörter durch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f5a7f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "with open('Downloads/1774-Werther.txt', 'r', encoding='utf-8') as input:\n",
    "    text = input.read()\n",
    "\n",
    "language = 'german'\n",
    "token = word_tokenize(text, language)\n",
    "\n",
    "# Wir verwenden eine List Comprehension, mit der nur die Token in die neue Liste\n",
    "# übernommen werden, die nicht in der Stopwords Liste sind.\n",
    "filtered_token = [t for t in token if t not in stopwords.words(language)]\n",
    "\n",
    "# Erneut erstellen wir eine vertikale Tokenliste, die in die neue Datei ausgegeben wird.\n",
    "clean_token = '\\n'.join(filtered_token)\n",
    "\n",
    "with open('Downloads/1774-Werther_ohneStopwords.txt', 'w', encoding='utf-8') as output:\n",
    "    output.write(clean_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bc0e36",
   "metadata": {},
   "source": [
    "Wenn Sie Stopwörter in einer anderen Sprache entfernen wollen, geben Sie in oben stehendem Code statt 'german' einfach die Sprache ein, die Sie benötigen. Um eine Liste aller Sprachen zu bekommen, für die NLTK Stopwortlisten bereitstellt, gehen Sie mit Ihrem Cursor in unten stehende Box und klicken Sie dann \"Run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05b06cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'bengali', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65142d62",
   "metadata": {},
   "source": [
    "### Entfernen von Satzeichen und Zahlen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf03202",
   "metadata": {},
   "source": [
    "Je nach Fragestellung, die Sie untersuchen wollen, kann es sinnvoll sein, Satzzeichen und Zahlen, also alle nicht-alphabetischen Elemente aus einem Text herauszufiltern. Wenn Sie Ihren Text oder Ihr Korpus auf diese Weise vorverarbeiten wollen, klicken Sie in die nächste Zelle und dann im Menü auf \"Run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "152657d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Downloads/1774-Werther.txt', 'r', encoding='utf-8') as input:\n",
    "    text = input.read()\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "token = word_tokenize(text)\n",
    "\n",
    "# Alle nicht-alphabetischen Zeichen werden herausgefiltert\n",
    "filtered_token = [t for t in token if t.isalpha()]\n",
    "clean_token = '\\n'.join(filtered_token)\n",
    "\n",
    "with open('Downloads/1774-Werther_ohneSatzzeichenundZahlen.txt', 'w', encoding='utf-8') as output:\n",
    "    output.write(clean_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133ad198",
   "metadata": {},
   "source": [
    "### Entfernen von Zahlen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc7a03b",
   "metadata": {},
   "source": [
    "Wenn Sie zwar Zahlen aber keine Satzzeichen entfernen wollen, klicken Sie in unten stehende Box und dann im Menü auf \"Run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "712725b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Downloads/1774-Werther.txt', 'r', encoding='utf-8') as input:\n",
    "    text = input.read()\n",
    "\n",
    "# Hier verwenden wir wieder die re.sub() Funktion\n",
    "text_without_numbers = re.sub(\n",
    "    '[0-9]',    # Der reguläre Ausdruck für Zahlen\n",
    "    '',\n",
    "    text\n",
    ")\n",
    "\n",
    "# Hier tokenisieren wir den bereinigten Text\n",
    "token = nltk.word_tokenize(text_without_numbers, 'german')\n",
    "clean_token = '\\n'.join(token)\n",
    "\n",
    "with open('Downloads/1774-Werther_ohneZahlen.txt', 'w', encoding='utf-8') as output:\n",
    "    output.write(clean_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f5ce1",
   "metadata": {},
   "source": [
    "### Entfernen von Satzzeichen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c735cb",
   "metadata": {},
   "source": [
    "Wenn Sie ausschließlich Satzzeichen aus Ihrem Text herausfilten möchten, klicken Sie in die nächste Zeile und dann oben auf \"Run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a83432dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "with open('Downloads/1774-Werther.txt', 'r', encoding='utf-8') as input:\n",
    "    text = input.read()\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "filtered_token = [\n",
    "    t for t in tokens\n",
    "    if t not in string.punctuation # Filtern der Interpunktionszeichen\n",
    "]\n",
    "\n",
    "clean_token = '\\n'.join(token)\n",
    "\n",
    "with open('Downloads/1774-Werther_ohneSatzzeichen.txt', 'w', encoding='utf-8') as output:\n",
    "    output.write(clean_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21893a86",
   "metadata": {},
   "source": [
    "### Entfernen kurzer Wörter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dbd4e1",
   "metadata": {},
   "source": [
    "Wörter, die nur aus drei Buchstaben oder weniger bestehen sind häufig Funktionswörter. Ein anderer Grund, aus dem Sie evtl. kurze Wörter bei der Vorverarbeitung Ihres Korpus herausfiltern möchten, ist, dass Fehler, die durch mangelhafte OCR-Erkennung entstanden sein können, auf diese Weise minimiert werden können. OCR-Software erkennt manchmal Bindestriche am Seitenrand nicht und \"zerhackt\" darum manchmal Wörter. \n",
    "\n",
    "Wenn Sie aus oben genannten oder anderen Gründen, Wörter einer bestimmten Länge aus Ihrem Text herausfiltern möchten, so können Sie das mit unten stehendem Code tun. Ändern Sie dafür den Dateipfad in der ersten Zeile, so, dass dieser zu Ihrem Text oder Textkorpus führt. Arbeiten Sie mit unserem Beispieltext, so kann es sein, dass der Dateipfad bereits korrekt eingegeben ist. \n",
    "\n",
    "Mit dem unten stehenden Code filtern Sie Wörter aus einem Text heraus, die kürzer als drei Wörter sind. Möchten Sie Wörter bis zu einer anderen Länge aus Ihrem Text herausholen, so ersetzen Sie einfach die 3. Wenn Sie lange Wörter herausfiltern möchten statt kurze, so ändern Sie das > Zeichen und machen Sie daraus <. Passen Sie in diesem Fall auch die Zahl an.\n",
    "\n",
    "Wenn Sie den Code angepasst haben, klicken Sie in die unten stehende Box und dann oben auf \"Run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d2afdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Downloads/1774-Werther.txt', 'r', encoding='utf-8') as input:\n",
    "    text = input.read()\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "filtered_token = [\n",
    "    t for t in tokens\n",
    "    if len(t) > 3       # Filtern nach der Tokenlänge\n",
    "]\n",
    "\n",
    "clean_token = '\\n'.join(token)\n",
    "\n",
    "with open('Downloads/1774-Werther_nurlangetoken.txt', 'w', encoding='utf-8') as output:\n",
    "    output.write(clean_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bdb6ab",
   "metadata": {},
   "source": [
    "### Entfernen kurzer Sätze\n",
    "\n",
    "Wenn Sie Sätze einer bestimmten Länge aus Ihrem Text herausfiltern möchten, so können Sie dies mit unten stehendem Code tun. Passen Sie dazu den Dateipfad in der ersten Zeile so an, dass dieser zu Ihrem Text oder Textkorpus führt. Wenn Sie mit unserem Beispieltext arbeiten, müssen Sie hier evtl. gar nichts anpassen. Mit diesem Code können Sie Sätze aus einem Text herausfiltern, die kürzer sind als 4 Wörter. Elemente wie \"Erster Theil\", die nicht zum eigentlichen Erzähltext gehören, sondern Formelemente sind, bekommen Sie damit aus Ihrem Text herausgerechnet. Bedenken Sie aber auch, dass derselbe Code auch Sätze wie \"Aber halt!\" aus dem Text herauswirft, obwohl diese zum eigentlichen Erzähltext gehören. Nutzen Sie diesen Vorverarbeitungsschritt also bewusst und vorsichtig!\n",
    "\n",
    "Im Folgenden Finden Sie drei Varianten des Codes, die Ihnen unterschiedliche Output-Dateien ausgeben. Das erste Beispiel gibt Ihnen einen Fließtext, in dem alle (übrig gebliebenen) Tokens und Sätze als solche markiert sind.\n",
    "\n",
    "#### Entfernen kurzer Sätze und Speichern als Fließtext\n",
    "\n",
    "Möchten Sie Sätze einer bestimmten Länge aus Ihrem Text herausrechnen und den Text in einer Datei als Fließtext abspeichern, in dem alle Wörter und Sätze als solche markiert sind, so klicken Sie in unten stehende Box und dann oben im Menü auf \"Run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "45fec493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "with open('Downloads/1774-Werther.txt', 'r', encoding='utf-8') as input:\n",
    "    text = input.read()\n",
    "\n",
    "sentences = nltk.sent_tokenize(text, 'german')\n",
    "long_sentences = [\n",
    "    sent for sent in sentences\n",
    "    if len(word_tokenize(sent, 'german')) > 4   # Filtern anhand der Tokenanzahl pro Satz\n",
    "]\n",
    "\n",
    "# Hier fügen wir alle getrennten Sätze wieder zusammen.\n",
    "long_sentences_str = ' '.joint(long_sentences)\n",
    "\n",
    "with open('Downloads/Werther_noShortSentences_Zeilen.txt', 'w', encoding='utf-8') as output:\n",
    "    output.write(long_sentences_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f424d2",
   "metadata": {},
   "source": [
    "#### Entfernen kurzer Sätze und Speichern mit einem ein Satz pro Zeile\n",
    "\n",
    "Möchten Sie Sätze einer bestimmten Länge aus Ihrem Text herausrechnen und den Text in einer Datei abspeichern, in der jeder Satz eine Zeile bildet und Wörter und Sätze nicht markiert sind, so klicken Sie in unten stehende Box und dann oben im Menü auf \"Run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a72bc229",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Downloads/1774-Werther.txt', 'r', encoding='utf-8') as input:\n",
    "    text = input.read()\n",
    "\n",
    "sentences = nltk.sent_tokenize(text, 'german')\n",
    "filtered_sentences = [s for s in sentences if len(nltk.word_tokenize(sent, 'german')) > 4]\n",
    "output_str = '\\n'.join(filtered_sentences)\n",
    "\n",
    "with open('Downloads/Werther_noShortSentences_Zeilen_ohneTokens.txt', 'w', encoding='utf-8') as output:\n",
    "    output.write(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b2307b",
   "metadata": {},
   "source": [
    "Sie haben nun einige zentrale Funktionen zur Vorverarbeitung (Preprocessing) Ihrer Texte für eine digitale Analyse kennengelernt. Sie können die Funktionen alle auch einzeln nutzen. Speichern Sie sich dieses Notebook also gerne ab und nutzen es immer dann, wenn Sie einzelne Funktionen davon zum Preprocessing brauchen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb4021",
   "metadata": {},
   "source": [
    "## Preprocessing Pipelines\n",
    "\n",
    "Um die Funktionen zu Textbereinigung, Wort-Tokenisierung und Satz-Tokenisierung übersichtlich und mit wenig Code zu verwenden,\n",
    "können wir auch Preprocessing Pipelines bauen.\n",
    "Wir stellen im folgenden 3 Pipeliens vor, die Sie nutzen und weiterentwickeln können: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05648f3",
   "metadata": {},
   "source": [
    "### Text Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c8dd087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des Regular Expression Package\n",
    "import re\n",
    "\n",
    "\n",
    "class TextPipeline:\n",
    "    def __init__(self, text_dir: str = 'Downloads/1774-Werther.txt'):\n",
    "        with open(text_dir, 'r', encoding='utf-8') as input:\n",
    "            self.text = input.read()\n",
    "\n",
    "    def remove_double_spaces(self):\n",
    "        self.text = re.sub(' +', ' ', self.text)\n",
    "\n",
    "    # hier kommen weitere Bereinigungsfunktionen für Texte ergänzt werden, liebe Mareike!\n",
    "\n",
    "    def save(self, file_name: str = 'clean_text.txt'):\n",
    "        with open(file_name, 'w', encoding='utf-8') as output:\n",
    "            output.write(self.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc11721",
   "metadata": {},
   "source": [
    "### Token Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b40eda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['muss', 'nicht']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "class TokenPipeline:\n",
    "    # Die Klasse \"TokenPipeline\" enthält alle Bereinigungsfunktionen, die oben erklärt wurden.\n",
    "    # Weitere Bereinigungsschritte können ergänzt werden.\n",
    "    # Im ersten Schritt der Token-Pipeline wird der Text geladen und tokenisiert.\n",
    "    def __init__(self, text_dir: str = 'Downloads/1774-Werther.txt', language: str = 'german'):\n",
    "        with open(text_dir, 'r', encoding='utf-8') as input:\n",
    "            text = input.read()\n",
    "        \n",
    "        self.tokens = word_tokenize(text, language)\n",
    "        self.language = language\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "        self.tokens = [t for t in self.tokens if t not in stopwords.words(self.language)]\n",
    "\n",
    "    def remove_short_token(self, min_len: int = 3):\n",
    "        self.tokens = [t for t in self.tokens if len(t) >= min_len]\n",
    "\n",
    "    def remove_numbers(self):\n",
    "        self.tokens = [t for t in self.tokens if not re.findall('[0-9]', t)]\n",
    "\n",
    "    def remove_interpunctation(self):\n",
    "        self.tokens = [t for t in self.tokens if t not in string.punctuation]\n",
    "\n",
    "    def save(self, file_name: str = 'clean_token.txt'):\n",
    "        token_str = '\\n'.join(self.tokens)\n",
    "        with open(file_name, 'w', encoding='utf-8') as output:\n",
    "            output.write(token_str)\n",
    "\n",
    "\n",
    "# Um die Token-Pipeline zu beginnen, geben wir den Dateinamen unseres Textes an:\n",
    "token_pipeline = TokenPipeline(text_dir='Downloads/1774-Werther.txt', language='german')\n",
    "\n",
    "# Jetzt können wir alle Bereinigungsschritte beliebig hinzufügen oder weglassen:\n",
    "token_pipeline.remove_numbers()\n",
    "token_pipeline.remove_interpunctation()\n",
    "token_pipeline.remove_stopwords()\n",
    "token_pipeline.remove_short_token(min_len=4)\n",
    "\n",
    "# Als letztes sichern wir die bereinigte Tokenliste und wählen dafür einen Dateinamen aus:\n",
    "token_pipeline.save(file_name='clean_token.txt')\n",
    "\n",
    "# Wir können uns das Ergebnis aber zur Überprüfung zusätzlich hier ausgeben lassen:\n",
    "print(token_pipeline.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b28545c",
   "metadata": {},
   "source": [
    "### Satz Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b72eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "class SentencePipeline:\n",
    "    def __init__(self, text_dir: str = 'Downloads/1774-Werther.txt', language: str = 'german'):\n",
    "        with open(text_dir, 'r', encoding='utf-8') as input:\n",
    "            text = input.read()\n",
    "        \n",
    "        self.sentences = sent_tokenize(text, language)\n",
    "        self.language = language\n",
    "\n",
    "    def remove_short_sentences(self, min_len: int = 3):\n",
    "        self.sentences = [\n",
    "            sent for sent in self.sentences\n",
    "            if len(word_tokenize(sent, self.lanuage)) > min_len\n",
    "        ]\n",
    "\n",
    "    # hier können weitere Bereinigungsfunktionen ergänzt werden.\n",
    "\n",
    "    def save_as_list(self, file_name: str = 'sententence_list.txt'):\n",
    "        sentence_str = '\\n'.join(self.sentences)\n",
    "        with open(file_name, 'w', encoding='utf-8') as output:\n",
    "            output.write(sentence_str)\n",
    "\n",
    "    def save_as_text(self, file_name: str = 'sententence_list.txt'):\n",
    "        sentence_str = ' '.join(self.sentences)\n",
    "        with open(file_name, 'w', encoding='utf-8') as output:\n",
    "            output.write(sentence_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
